\subsection{Maximizing Likelihood for a Three Step Markov Process}

In this example we will look at deriving statistical estimates for a three step Markov process. For
more background information, see chapter one of \cite{Sullivant}; however \cite{Sullivant} is missing
a derivation of the final formula.

\subsubsection*{The Setup : The Constraints}

\newcommand{\flip}[1]{\overline{#1}}

We have three random variables \(X_1\), \(X_2\), and \(X_3\) where each \(X_i = 0\) or \(X_i = 1\). The order
of the variables matter, and we think of them as randomly being chosen in sequence according to their indices
one, two, or three. 

So there are eight possible outcomes according to the two possible values for each \(X_i\); we label these
outcomes as \(X_1X_2X_3\), e.g. \(000\) or \(110\). We label the probabilities of the outcomes according to these  
possibilities, e.g. \(p_{000}\) or \(p_{110}\). We think of the probabilities forming a vector 
\(\vec p \in \mathbb R^8\), i.e. the vector \(\vec p = (p_{000}, p_{001}, ..., p_{111})\). 

Finally, one more notation we will use. We will use \(\flip i\) to denote the other value of \(0\) or \(1\) that 
is not \(i\); i.e. if \(i = 1\) then \(\flip i = 0\).

To be a three step Markov process, the transition from \(X_2\) to \(X_3\) needs to depend only on \(X_2\) and
not on the entire history, i.e. not depend on \(X_1\) and \(X_2\). In terms of probabilities, this is
expressed as 
\begin{equation}
P(X_3 = k | X_1 = i, X_2 = j) = P(X_3 = k | X_2 = j).
\end{equation}
Applying Bayes' formula to both sides of this equation, we get
\begin{equation}
\frac{p_{ijk}}{\sum_\gamma p_{ij\gamma}} 
= \frac{\sum_\alpha p_{\alpha jk}} {\sum_{\alpha, \gamma} p_{\alpha j\gamma} }.
\end{equation}
We can rewrite this as 
\begin{equation}
p_{ijk} \sum_{\alpha, \gamma} p_{\alpha j\gamma} =
     \left(\sum_\alpha p_{\alpha jk}\right) \left( \sum_\gamma p_{ij\gamma} \right).
\end{equation}
Next, let's expand the sums over \(\gamma\) as sums over the values \(k\) and \(\flip k\); we get
\begin{equation}
p_{ijk}\sum_\alpha p_{\alpha jk} + p_{ijk} \sum_\alpha p_{\alpha j\flip k} =
p_{ijk} \sum_\alpha p_{\alpha jk} + p_{ij\flip k} \sum_\alpha p_{\alpha jk}.
\end{equation}
Cancelling terms we get
\begin{equation}
p_{ijk} \sum_\alpha p_{\alpha j\flip k} = p_{ij\flip k} \sum_\alpha p_{\alpha jk}.
\end{equation}
Now expand the sum over \(\alpha\) as a sum over the values \(i\) and \(\flip i\), we get
\begin{equation}
p_{ijk} p_{ij\flip k} + p_{ijk} p_{\flip ij\flip k} = p_{ij\flip k} p_{ijk} + p_{ij\flip k} p_{\flip ijk}.
\end{equation}
Again, cancelling terms we get
\begin{equation}
p_{ijk} p_{\flip ij\flip k} = p_{ij\flip k} p_{\flip ijk}.
\end{equation}

Now, at first this appears to be eight different equations, one for each possible choice of \((i, j, k)\). 
However, we will now show that it is actually just two different equations without doing a brute force
plug and check.

First, notice that the equation is exactly the same if we make the substitution \(i \to \flip i\); the effect
is to merely switch the left and right hand side of the equation. Therefore, the equation is the same no matter
which value of \(i\) we choose. So let us choose \(i = 0\).

Similarly, using the substitution \(k \to \flip k\), we can choose \(k = 0\). Both of these choices give
\begin{equation}
p_{0j0} p_{1j1} = p_{0j1} p_{1j0},
\end{equation}
for either \(j = 0\) or \(j = 1\). It is not very hard to see that we get different equations for each
different value of \(j\). 

Therefore, we find that the constraints for \(\vec p\) to be a three step Markov process are exactly the
four following constraints: 
\begin{equation}
\begin{cases}
p_{ijk} \geq 0, \\
\sum_{\alpha, \beta, \gamma} p_{\alpha\beta\gamma} = 1, \\
p_{000} p_{101} = p_{001} p_{100}, \\ 
p_{010} p_{111} = p_{011} p_{110}.
\end{cases}
\end{equation}
All probability vectors \(\vec p \in \mathbb R^8\) that belong to three step Markov processes are exactly 
the probability vectors \(\vec p\in \mathbb R^8\) that satisfy all of the above constraints. 

\subsection*{The Setup: Maximizing Likelihood}

We are interested in creating statistical estimates for the different \(p_{ijk}\) based on data recording
sample counts \(n_{ijk}\); that is, we run \(N\) independent trials, and \(n_{ijk}\) is the 
number of times we see outcome \(X_1 = i\), \(X_2 = j\), and \(X_3 = k\). We denote the collection of
all the \(n_{ijk}\) as a vector \(\vec n\) similarly to how we used \(\vec p\) above. 

First, let us briefly discuss the notion of likelihood. For the notion of likelihood, you consider the
data \(\vec n\) to be fixed, and we consider varying the probabilities of our model \(\vec p\). The 
likelihood is defined as the probabilitiy \(l(\vec p) = P(\vec n | \vec p) \) for our three step Markov
model. Assuming the trials are independent, we have
\begin{equation}
l(\vec p) = \prod_{\alpha, \beta, \gamma} (p_{\alpha\beta\gamma})^{n_{\alpha\beta\gamma}}.
\end{equation} 
The term "likelihood" is used instead of probability, because \(l(\vec p)\) does not in general represent
a probability distribution on \(\vec p\).

The idea is that a good estimate of the true probabilities should come from finding \(\vec p\) that
maximizes the likelihood \(l(\vec p\). 

Next note that maximizing likelihood is equivalent to maiximizing the logarithm of likelihood; however,
the latter has a nicer form. So let \(L(\vec p) = \log(l(\vec p))\). We see that
\begin{equation}
L(\vec p) = \sum_{\alpha, \beta, \gamma} n_{\alpha\beta\gamma} \log(p_{\alpha\beta\gamma}).
\end{equation}

Now, recall that those \(\vec p\) that represent three step Markov processes are exactly those \(\vec p\) 
satisfying four constraints. So we are lead to a constrained maximization problem. We will assume that
the maximum occurs at the interior of the constraints, i.e. \(p_{ijk} > 0\) for all \((i, j, k)\). 

\subsubsection*{The Problem}

Let the data \(n_{ijk}\) be fixed. Assume that the maximum of the following constrained problem occurs at \(p_{ijk} > 0\) for all \((i, j, k)\):
\begin{equation}
\begin{cases}
\text{maximize } L(\vec p) = \sum_{\alpha, \beta, \gamma} n_{\alpha\beta\gamma} \log p_{\alpha\beta\gamma}, \\
\sum_{\alpha, \beta, \gamma} p_{\alpha\beta\gamma} = 1, \\
p_{0j0} p_{1j1} - p_{0j1} p_{1j0} = 0, & \text{ for } j \in \{0, 1\}.
\end{cases}
\end{equation}

Find the \(p_{ijk}\) where the maximum occurs in terms of the data \(n_{ijk}\).

\subsubsection*{The Solution}

For convenience of notation, let us make the following definitions
\begin{align}
f(\vec p) & \coloneqq \sum_{\alpha, \beta, \gamma} p_{\alpha\beta\gamma}, \\
g_j(\vec p) & \coloneqq p_{0j0} p_{1j1} - p_{0j1} p_{1j0}.
\end{align}

Let us look the Lagrangian condition for finding the constrained critical points of \(L(\vec p)\).
Now, note that 
\begin{align}
\frac{\partial L}{\partial p_{ijk}} & = \frac{n_{ijk}} {p_{ijk}}, \\
\frac{\partial f}{\partial p_{ijk}} & = 1,
\end{align}
for all \((i, j, k)\). Next, let us consider the derivatives of \(g_j(\vec p)\). We see that
\begin{align}
\frac{\partial g_j}{\partial p_{ijk}} & = (-1)^{i + k} p_{\flip i j\flip k}, \\
\frac{\partial g_j}{\partial p_{i\flip jk} } & = 0.
\end{align}

Now, let \(\lambda\) be the Lagrangian coefficient for \(f(\vec p)\) and let the two coefficients \(\tau_j\)
be the Lagrangian coefficients for \(g_j(\vec p)\). The Lagrangian condition gives us that
\begin{equation}
\frac{n_{ijk}} {p_{ijk}} = \lambda + \tau_j (-1)^{i + k} p_{\flip i j\flip k},
\end{equation}
for each \((i, j, k)\). Note how coefficient \(\tau_0\) and variables \(p_{i0k}\) are 
decoupled from \(\tau_1\) and \(p_{i1k}\); that is no equation has some variable or coefficient from both sets. 
However, \(\lambda\) is coupled to all of them.

Now, multiply through to get
\begin{equation}
n_{ijk} = \lambda p_{ijk} + \tau_j (-1)^{i + k} p_{ijk} p_{\flip ij\flip k}.
\end{equation}
Next consider this equation for \((i, j, \flip k)\) and note that \((-1)^{i+k} = -(-1)^{i + \flip k}\). So
we have
\begin{equation}
n_{ij\flip k} = \lambda p_{ij\flip k} - \tau_j (-1)^{i + k} p_{ij\flip k} p_{\flip ijk}.
\end{equation}
Next, use the constraint \(p_{ijk}p_{\flip ij\flip k} = p_{ij\flip k}p_{\flip ijk}\) and add together the 
two equations to get
\begin{equation}
n_{ijk} + n_{ij\flip k} = \lambda(p_{ijk} + p_{ij\flip k}).
\end{equation}

Similarly do the same for \((\flip i, j, k)\), and we obtain
\begin{equation}
\frac{n_{ijk} + n_{\flip ijk}} {p_{ijk} + p_{\flip ijk}} = 
    \frac{n_{ijk} + n_{ij\flip k}} {p_{ijk} + p_{ij\flip k}} =
    \lambda.
\end{equation}

Let us concentrate on the following constraints:
\begin{equation}
\begin{cases}
\sum_{\alpha, \beta, \gamma} p_{\alpha\beta\gamma} = 1, \\
\frac{n_{0jk} + n_{1jk}} {p_{0jk} + p_{1jk}} = 
    \frac{n_{ij0} + n_{ij1}} {p_{ij0} + p_{ij1}} =
    \lambda, \\
p_{0j0}p_{1j1} = p_{0j1}p_{1j0}.
\end{cases}
\end{equation}
Note that these constraints are invariant under the substitution \(i \to \flip i\) (appropriately interpreted
for the quadratic constraint); that is the substitution \(i \to \flip i\) is a symmetry for these constraints. 
Similarly \(j \to \flip j\) is a symmetry as well.

We will want to use these symmetries to help us solve these system of constraints. Before we do so, let us
show how to formalize these symmetries.

Let \(S(\vec p)_{ijk} = p_{\flip ijk}\) be the linear transformation that involves switching components
according to the substitution \(i \to \flip i\). Similarly let \(T(\vec p)_{ijk} = p_{ij\flip k}\) be that
corresponding to the substitution \(j \to \flip j\). 

A symmetry in the substitution \(i \to \flip i\) means that \(\vec p\) satisfies these constraints 
if and only if \(S (\vec p)\) does too; similarly for a symmetry in the substitution \(j \to \flip j\) and the
transformation \(T\).

The key point is that these constraints are easier to understand if we change to coordinates that are special
for the transformations \(S\) and \(T\). Note that \(ST = TS\) and they are both orthogonal transformations, 
and so we can decompose \(\mathbb R^8\) into an eigenbasis for both \(S\) and \(T\); if you are uncomfortable with
these theoretical details, then it is enough to know that we want to try and we will see that it will work. 

First, let us decompose into an eigenbasis of \(S\). Note that \(S\) just switches the components of \(\vec p\)
in pairs, and so \(S^2 = 1\). Therefore the eigenvalues of \(S\) are at most \(\pm 1\). In fact, it is 
simple to correctly guess the eigenvectors of \(S\). This leads us to an intial change of variables
\begin{align}
q_{+jk} & \coloneqq p_{0jk} + p_{1jk}, \\
q_{-jk} & \coloneqq p_{0jk} - p_{1jk}.
\end{align}
Note that we have used subscripts of \(\pm\) to indicate whether the variable corresponds to 
eigenvalue \(\pm 1\).
Also take note of the symmetry for the substitution \(p_{ijk} \to p_{\flip ijk}\) in the 
definition of \(q_{+jk}\); furthermore, such a substitution in the definition of \(q_{-jk}\) is 
an anti-symmetry, i.e. it changes the sign.

Next, we wish to further decompose for \(T\). Again, it is simple enough to guess at the right answer. The 
coordinates are
\begin{align}
r_{+j+} & \coloneqq p_{0j0} + p_{1j0} + p_{0j1} + p_{1j1}, \\
r_{+j-} & \coloneqq p_{0j0} + p_{1j0} - p_{0j1} - p_{1j1}, \\
r_{-j+} & \coloneqq p_{0j0} - p_{1j0} + p_{0j1} - p_{1j1}, \\
r_{-j-} & \coloneqq p_{0j0} - p_{1j0} - p_{0j1} + p_{1j1}, \\
\end{align}
Again, note the symmetries and anti-symmetries for the substitution \(p_{ijk} \to p_{\flip ijk}\) and for the
substitution \(p_{ijk} \to p_{ij\flip k}\).

It will be convenient if we do a similar change of coordinates to \(n_{ijk}\) to make coordinates 
\(\{m_{+j+}, m_{+j-}, m_{-j+}, m_{-j-}\}\).

Next, we rewrite our constraints in terms of these new coordinates.

First, we note that \(\sum_{\alpha, \beta, \gamma} p_{\alpha\beta\gamma} = \sum_j r_{+j+}\), so we get 
the constraint
\begin{equation}
r_{+0+} + r_{+1+} = 1.
\end{equation}

Next, let's look at the linear constraints \(n_{ij0} + n_{ij1} = \lambda (p_{ij0} + p_{ij1})\) for each 
value of \(i\). We note
that both sides are invariant under the substitution \(k \to \flip k\). So we seek expressing the right side
in terms of \(r_{+j+}\) and \(r_{-j+}\), and similar expressions for the left hand side.

We immediately see that \(r_{+j+} + r_{-j+} = 2(p_{0j0} + p_{0j1})\) and 
that \(r_{+j+} - r_{-j+} = 2(p_{1j0} + p_{1j1})\). We get a similar result for
\(m_{+j+} + m_{-j+}\) and \(m_{+j+} - m_{-j+}\). Therefore we have
\begin{align}
m_{+j+} + m_{-j+} & = \lambda(r_{+j+} + r_{-j+}), \\
m_{+j+} - m_{-j+} & = \lambda(r_{+j+} - r_{-j+}), \\
\end{align}

So we get
\begin{align}
m_{+j+} & = \lambda r_{+j+}, \\
m_{-j+} & = \lambda r_{-j+}.
\end{align}

Similarly, using the linear constraints \(n_{0jk} + n_{1jk} = \lambda (p_{0jk} + p_{1jk})\) for each value of 
\(k\), we get another equation (but only one new equation):
\begin{equation}
m_{+j-} = \lambda r_{+j-}.
\end{equation}

Now, it is easy to see that \(N = m_{+0+} + m_{+1+}\). So using our new equations we get
\begin{align}
N & = \lambda(r_{+0+} + r_{+1+}), \\
& = \lambda. 
\end{align}

So we get
\begin{align}
r_{+j+} & = \frac{m_{+j+}} {N}, \\
r_{-j+} & = \frac{m_{-j+}} {N}, \\
r_{+j-} & = \frac{m_{+j-}} {N}.
\end{align}
Note that the right hand sides only depend on the data \(n_{ijk}\), which is fixed. In order to
obtain our original \(p_{ijk}\), we still need to find \(r_{-j-}\) in terms of the data. 

To do so, we will have to use the quadratic equations \(p_{0j0}p_{1j1} - p_{0j1}p_{1j0} = 0\). 
We need to rewrite this quadratic polynomial in \(p_{ijk}\) as a quadratic polynomial in \(r\)-coordinates.
For now, forget the other constraints and just consider these quadratic equations for 
any \(\vec p \in \mathbb R^8\).

Recall that these equations are described using the quadratic forms \(g_j(\vec p)\). 
Next, note that the quadratic forms \(g_j\) are anti-symmetric for the substitution 
\(p_{ijk} \to p_{\flip ijk}\)
and for the substitution \(p_{ijk} \to p_{ij\flip k}\).

These substitutions correspond to our transformations \(S\) and \(T\). So when we find how \(g_j\) depends
on the coordinates \(r_{+j+}\), \(r_{+j-}\), \(r_{-j+}\), and \(r_{-j-}\), we can make our work much 
shorter by paying attention to the anti-symmetries of the substitutions.

For example, we know that \(g_j\) will be a quadratic polynomial in the \(r\)-coordinates. Let us consider
what the coefficient of \(r_{+j+}r_{+j+}\) can be. So consider
\begin{equation}
g_j(\vec p) = A r_{+j+}r_{+j+} + ...
\end{equation} 
From the above discussion we know that \(g_j(S\vec p) = g_j(s\vec p)\). However, the \(r_{+j+}\) coordinates
of \(S\vec p\) is the same as \(\vec p\). Therefore we get
\begin{align}
-(A r_{+j+}r_{+j+} + ...) & = -g_j(\vec p), \\
& = g_j(S\vec p), \\
& = A r_{+j+}r_{+j+} \pm ....
\end{align}
Now, we haven't discussed what happens to the other \(r\)-coordinates, but it doesn't matter as none of them
transform into having any \(r_{+j+}\) coordinate. Therefore, we have \(-A = A\), and so we get \(A = 0\).
Hence the coefficient of \(r_{+j+}r_{+j+}\) is zero. The key for why this worked out so nicely is that the
\(r\)-coordinates are constructed to from the eigenvectors of both \(S\) and \(T\). 

In fact, we can use the anti-symmetries to narrow down our list of which terms can be non-zero. They need
to match the anti-symmetry of the substitutions. Consider first the substitution \(p_{ijk} \to p_{\flip ijk}\).
This results in an anti-symmetry means that we can only have non-zero coefficients for those terms that
have exactly one minus in their \(i\) spot, i.e. \(r_{+jx}r_{-jy}\) where \(x,y \in \{-, +\}\).

Similarly, considering the anti-symmetry of the substitution \(p_{ijk} \to p_{ij\flip k}\), we narrow
down the terms to
\begin{equation}
g_j(\vec p) = B r_{+j+}r_{-j-} + C r_{+j-}r_{-j+},
\end{equation}
for some unknown constants \(B, C\).

To find \(B\) and \(C\), we simply substitute in some choice of \(\vec p^0\) (recall that we our now considering
all \(\vec p\in\mathbb R^8\)). Let us use \(p^0_{0j0} = s\), \(p^0_{1j1} = t\), and all other 
values of \(p^0_{ijk} = 0\). We have
\begin{align}
st & = p^0_{0j0}p^0_{1j1} - p^0_{0j1} p^0_{1j0}, \\
r^0_{+j+} & = s + t, \\
r^0_{+j-} & = s - t, \\
r^0_{-j+} & = s - t, \\
r^0_{-j-} & = s + t.
\end{align}
So we get
\begin{align}
Ar^0_{+j+}r^0_{-j-} + Br^0_{+j-}r^0_{-j+} & = A(s + t)^2 + B(s - t)^2, \\
& = (A + B)s^2 + 2(A - B)st + (A + B)t^2. 
\end{align}
So we get
\begin{align}
A + B & = 0, \\
2(A - B) & = 1
\end{align}
Therefore, we get \(A = 1/4\) and \(B = -1/4\). So
\begin{equation}
g_j = \frac{r_{+j+}r_{-j-} - r_{+j-}r_{-j+} } {4}.
\end{equation}

So the condition that \(p_{0j0}p_{1j1} = p_{0j1} p_{1j0}\) becomes 
\(r_{+j+} r_{-j-} = r_{+j-} r_{-j+} \). Therefore, we can solve for \(r_{-j-}\) to
get
\begin{equation}
r_{-j-} = \frac{m_{+j-} m_{-j+} }{N m_{+j+} }.
\end{equation}

Now that we have solved the \(r\)-coordinates in terms of the data, we can solve
for the \(p_{ijk}\). Again, we can pay attention to the symmetries to minimize the
amount of work this entails.

First, let us solve for \(p_{0j0}\). We note that
\begin{align}
p_{0j0} & = \frac{r_{+j+} + r_{+j-} + r_{-j+} + r_{-j-} } {4}, \\
& = \frac{m_{+j+}( m_{+j+} + m_{+j-} + m_{-j+}) + m_{+j-}m_{-j+}} {4Nm_{+j+}}, \\
& = \frac{m_{+j+}( m_{+j+} + m_{+j-}) + m_{-j+}(m_{+j+} + m_{+j-}) } {4Nm_{+j+}}, \\
& = \frac{(m_{+j+} + m_{-j+}) (m_{+j+} + m_{+j-}) } {4Nm_{+j+}}, \\
& = \frac{4(n_{0j0} + n_{0j1})(n_{0j0} + n_{1j0}) } {4Nm_{+j+}}, \\
& = \frac{\left(\sum_\gamma n_{0j\gamma}\right) \left(\sum_\alpha n_{\alpha j0}\right) } 
    {N\sum_{\alpha, \gamma} n_{\alpha j\gamma}}.
\end{align}

Now that we have solved for \(p_{0j0}\), we can use symmetries to solve for the rest of the \(p_{ijk}\). Let
\(\tilde n_{ijk} = n_{\flip ijk} \). Note that this amounts to relabeling the outcomes of \(X_1\). 
Next, let \(\tilde p_{ijk}\) be the optimal probabilities for \(\tilde n_{ijk}\). Since we are really
only relabeling \(X_1\), we have that \(\tilde p_{ijk} = p_{\flip ijk}\). 

Now we use the formula for \(\tilde p_{0j0}\). We get
\begin{align}
p_{1j0} & = \tilde p_{0j0}, \\
& = \frac{\left(\sum_\gamma \tilde n_{0j\gamma}\right) 
        \left(\sum_\alpha \tilde n_{\alpha j0}\right) }
        {N \sum_{\alpha, \gamma} \tilde n_{\alpha j\gamma}}, \\
& = \frac{\left(\sum_\gamma n_{1j\gamma}\right) 
        \left(\sum_\alpha n_{\alpha j0}\right) }
        {N \sum_{\alpha, \gamma} n_{\alpha j\gamma}}.
\end{align} 
Similarly we can compute every \(p_{ijk}\) as
\begin{equation}
p_{ijk} = \frac{\left(\sum_\gamma n_{ij\gamma}\right)
                \left(\sum_\alpha n_{\alpha jk}\right) }
               {N \sum_{\alpha, \gamma} n_{\alpha j\gamma}}.
\end{equation}
